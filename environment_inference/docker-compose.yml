services:
  qwen3-vl-inference:
    build:
      context: .
      dockerfile: Dockerfile
    image: qwen3-vl-inference:latest
    shm_size: '2gb'
    container_name: qwen3-vl-inference-container
    volumes:
      - /home/shimosaba/python_project/Qwen3-VL:/workspace
      - /home/shimosaba/python_project/internvl_chat/input:/input
      - /home/shimosaba/python_project/Qwen3-VL/qwen-vl-finetune/.triton:/.triton
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 64G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    tty: true
    stdin_open: true